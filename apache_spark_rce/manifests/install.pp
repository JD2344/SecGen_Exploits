# Class: apache_spark_rce
# apache_spark_install
#
class apache_spark_rce::install {
  Exec { path => [ '/bin/', '/sbin/' , '/usr/bin/', '/usr/sbin/' ] }

  # Install required packages
  # NOTE: once Debian updates insert scala 2.12+ into statement
  ensure_packages(['openjdk-11-jdk'], { ensure => 'installed'})

  # TODO: FIND proper way of getting spark to persist.
  # set environment on install - possible usage of systemd.service file.
  # echo 'export PATH=${PATH}:/usr/local/spark/bin' >> ~/.bashrc
  $releasename = 'spark-3.1.2-bin-hadoop3.2'
  $scaladeb = 'scala-2.12.10.deb'
  file { "/tmp/${releasename}.tgz":
    ensure => file,
    source => "puppet:///modules/apache_spark_rce/${releasename}.tgz"
  }
  -> file { "/tmp/${scaladeb}":
    ensure => file,
    source => "puppet:///modules/apache_spark_rce/${scaladeb}"
  }
  -> exec { 'unpack-spark':
    cwd     => '/tmp',
    command => "tar -xf ${releasename}.tgz",
    creates => '/tmp/spark'
  }
  -> exec { 'move-spark':
    cwd     => '/tmp',
    command => "mv /tmp/${releasename} /usr/local/spark/",
    creates => '/usr/local/spark',
  }
  -> exec { 'spark-set-acls':
    cwd     => '/usr/local/spark/conf/',
    command => 'echo "spark.acls.enable true" >> /usr/local/spark/conf/spark-defaults.conf',
  }
  -> package { 'scala':
    ensure   => latest,
    provider => apt,
    source   => "/tmp/${scaladeb}",
  }
}
