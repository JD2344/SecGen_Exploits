# Class: apache_spark_rce
# apache_spark_install
#
class apache_spark_rce::install {
  Exec { path => [ '/bin/', '/sbin/' , '/usr/bin/', '/usr/sbin/' ] }

  # Install required packages
  ensure_packages(['openjdk-11-jdk'], { ensure => 'installed'})

  #TODO: MAKE SURE SPARK PERSISTS (In testing, it had to be manually started...)
  $releasename = 'spark-3.1.2-bin-hadoop3.2'
  file { "/tmp/${releasename}.tgz":
    ensure => file,
    source => "puppet:///modules/apache_spark_rce/${releasename}.tgz"
  }
  -> exec { 'unpack-spark':
    cwd     => '/tmp',
    command => "tar -xf ${releasename}",
    creates => '/tmp/spark'
  }
  -> exec { 'move-spark':
    cwd     => '/tmp',
    command => 'mv /tmp/spark /usr/local/spark',
    creates => '/usr/local/spark',
  }
  -> exec { 'set-spark-path-env':
    cwd     => '~/',
    command => "echo 'export PATH=${PATH}:/usr/local/spark/bin' >> ~/.bashrc",
  }
}
